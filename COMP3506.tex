\documentclass[landscape]{cheat}
\usepackage{amsmath}
\usepackage{listings}

\begin{document}
\footnotesize
\begin{multicols}{3}

\begin{center}
\Large{\underline{COMP3506 Cheat Sheet}} \\
\end{center}


%% START HERE
\section{Complexities}

\subsection{List of complexities, in sorted order}
\begin{description}
    \item[$O(1)$] Constant time.
    \item[$O(\log(n))$] Logarithmic time.
    \item[$O(\log(n)^c)$] Polylogarithmic time.
    \item[$O(n)$] Linear time.
    \item[$O(n^c)$] Polynomial time.
    \item[$O(c^n)$] Exponential time.
\end{description}

\subsection{Definitions of complexities}
\begin{align*}
    f(n) = O(g(n)) \rightarrow& g(n) \geq f(n) \\
    f(n) = \Theta(g(n)) \rightarrow& g(n) = f(n) \\
    f(n) = \Omega(g(n)) \rightarrow& g(n) \leq f(n)
\end{align*}

\subsection{Recurrence Relations}
\begin{align*}
    T(n) =& a T(\frac n b) + f(n) \\
    f(n) = O(n^{\log_b a}) \rightarrow& T(n) \in \Theta(n^{\log_b a}) \\
    f(n) \in \Theta(n^{\log_b a} \log^k n) \rightarrow& T(n) \in \Theta(n^{\log_b a} \log^{k+1} n) \\
    \frac {f(n) = \Omega(n^{\log_b a})} {af(\frac n b) \leq f(n)} \rightarrow& T(n) \in \Theta(f(n)) \\
\end{align*}

\section{Sorting Algorithms}
\begin{description}
    \item[Selection]
        \textsc{Scan} for the largest element.
        \textsc{Swap} largest element with last element.
        \textsc{Sort} elements 0 to $n-1$.
    \item[Merge]
        \textsc{Sort} each half of the list.
        \textsc{Merge} the two sorted halves together.
    \item[Quick]
        \textsc{Pick} a random pivot point.
        \textsc{Partition} the list, ensuring all elements are placed before the pivot iff they are less than it.
        \textsc{Sort} the list of elements larger than the pivot, and the list of elements smaller than it.
    \item[Bucket]
        \textsc{For} $i \in [0..l)$ where $l$ is the number of digits in the largest element.
        \textsc{Partition} the list elements into buckets, based off the value of their $i$th digit.
        \textsc{Combine} the buckets into a single list.
    \item[Heap]
        \textsc{Insert} all elements of the list into a priority queue.
        \textsc{Remove} all elements of the list from the priority queue, now in order.
\end{description}

\section{Linear Structures}

\subsection{Abstract data types}
\begin{description}
    \item[Stack]
        first in last out,
        supports \textsc{push}, \textsc{pop}, \textsc{top} and \textsc{size}.
    \item[Queue]
        first in first out,
        supports \textsc{enqueue}, \textsc{dequeue}, \textsc{first} and \textsc{size}.
    \item[List]
        sequence of elements,
        supports \textsc{get}, \textsc{set}, \textsc{add}, \textsc{remove} and \textsc{size}.
    \item[Positional List]
        sequence of keys that are associated with elements,
        supports \textsc{before}, \textsc{after}, \textsc{first}, \textsc{last}, \textsc{size},
        \textsc{addFirst}, \textsc{addLast}, \textsc{addBefore}, \textsc{addAfter}, \textsc{set} and \textsc{remove}.
\end{description}

\subsection{Concrete data types}
\begin{description}
    \item[Linked List]
        \textbf{O(1)} iteration, and insertion / removal at the current index.
        \textbf{O(n)} indexing, and insertion / removal at a given index.
    \item[Array]
        \textbf{O(1)} iteration, indexing, and inserting / removal at the end of the list.
        \textbf{O(n)} insertion / removal at a given index.
    \item[Skip List]
        \textbf{O(1)} iteration.
        \textbf{Expected O(log n)} indexing, search, insertion / removal at a given index.
        \textbf{Worst case O(n)} indexing, search, insertion / removal at a given index.
\end{description}

\section{Trees}

\subsection{Abstract data types}
\begin{description}
    \item[Sorted Set]
        key store,
        supports \textsc{largest}, \textsc{smallest}, \textsc{before}, \textsc{after},
        \textsc{exists}, \textsc{add}, \textsc{remove} and \textsc{size}.
    \item[Sorted Map]
        key-value store,
        supports \textsc{largest}, \textsc{smallest}, \textsc{before}, \textsc{after},
        \textsc{get}, \textsc{put}, \textsc{remove} and \textsc{size}.
\end{description}

\subsection{Complexities}
\begin{description}
    \item[Largest] $O(\log n)$ average case, $O(n)$ worst case.
    \item[Smallest] $O(\log n)$ average case, $O(n)$ worst case.
    \item[Before] $O(\log n)$ average case, $O(n)$ worst case.
    \item[After] $O(\log n)$ average case, $O(n)$ worst case.
    \item[Exists / Get] $O(\log n)$ average case, $O(n)$ worst case.
    \item[Add / Put] $O(\log n)$ average case, $O(n)$ worst case.
    \item[Remove] $O(\log n)$ average case, $O(n)$ worst case.
    \item[Size] $O(1)$ worst case.
\end{description}

\subsection{Balancing}
The difference between average case and worst case depends on whether the tree is balanced.
Different algorithms exist to balance trees, based on different invariants.

\begin{description}
    \item[Red-Black]
        All nodes are coloured either red or black.
        \textsc{Root} is black.
        \textsc{Leafs} (nil) are black.
        \textsc{Red} nodes have two black children.
        \textsc{All} paths from the root to a leaf have the same number of black nodes.
    \item[AVL]
        At each node, the height of the left and right subtrees must differ by no more than one.
        If an insertion or deletion breaks this invariant at a node, balancing the first node with a broken invariant will balance the tree.
    \item[B]
        All keys are stored at the same depth, in leaf nodes.
        All nodes except the root are guaranteed to have between $0.4B$ and $B$ elements in them.
        To maintain this invariant, nodes can be split in two, or combined.
    \item[$(a,b)$]
        Like $B$ trees, except instead of $0.4B$ and $B$ its $a$ and $b$.
        Height is $\Omega(\log_b n)$ and $O(\log_a n)$.
        Must maintain $2 \leq a \leq \frac {b+1} 2$.
    \item[Splay]
        Keeps the most frequently used nodes close to the root of the tree.
        \textsc{Insert} operations move the new node to the root of the tree.
        \textsc{Successful} searches move the found node to the root of the tree.
        \textsc{Unsuccessful} searches move the last node involved in the search to the root of the tree.
        \textsc{Removals} move the parent of the removed item to the root of the tree.
\end{description}

\subsection{Traversals}
\begin{description}
    \item[Pre-order] Visit \textsc{root}, \textsc{left}, \textsc{right}.
    \item[Post-order] Visit \textsc{left}, \textsc{right}, \textsc{root}.
    \item[In-order] Visit \textsc{left}, \textsc{root}, \textsc{right}.
    \item[Level-order] Breadth first search. Use a queue to store nodes and figure out when to visit nodes.
    \item[Euler] Visit \textsc{left}, \textsc{right}, \textsc{parent}.
\end{description}

\section{Heaps}
Also called a priority queue.
Stores a set of elements that are going to be accessed in sorted order.
Supports the following operations.

\begin{description}
    \item[Min] $\Theta(1)$.
    \item[RemoveMin] $\Theta(\log n)$.
    \item[Insert] $O(\log n)$ for binary heap, otherwise $\Theta(1)$.
    \item[Merge] $\Theta(n)$ for binary heap, $O(\log n)$ for binomial heap of $\Theta(1)$ for Fibonacci heap.
\end{description}

\subsection{Binary Heap}
A binary heap is a binary tree that satisfies the \textsc{heap order} property.
For every internal node $v$ except the root, $key(v) \geq key(parent(v))$.
A binary heap tree can have no gaps.

\section{Hash Tables}

\subsection{Abstract Data Types}
\begin{description}
    \item[Unsorted Set] key store,
        supports \textsc{exists}, \textsc{add}, \textsc{remove} and \textsc{size}.
    \item[Unsorted Map] key-value store,
        supports \textsc{get}, \textsc{put}, \textsc{remove} and \textsc{size}.
\end{description}

\subsection{Complexities}
\begin{description}
    \item[Exists / Get] $O(1)$ expected case, $O(n)$ worst case.
    \item[Add / Put] $O(1)$ expected case, $O(n)$ worst case.
    \item[Remove] $O(1)$ expected case, $O(n)$ worst case.
    \item[Size] $O(1)$ worst case.
\end{description}

\subsection{Collisions}
The difference between expected case and worst case depends on how many collisions the hashmap has, and how they are handled.
Different algorithms exist to handle the cases where hashes collide.
The best way to limit the effect of collisions in a hash table is to keep the load factor low.
\begin{description}
    \item[Chaining] Replace the buckets with linked lists of all keys which have the same compressed hash.
    \item[Linear Probing] Replace $h(k)$ with $h(k, i) = h(k) + i$ for $i \in (0,N]$ where $N$ is the size of the hashmap.
    \item[Quadtratic Probing] Replace $h(k)$ with $h(k, i) = h(k) + ai + bi^2$ for $i \in (0,N]$ where $N$ is the size of the hashmap.
    \item[Double Hashing] Replace $h(k)$ with $h(k, i) = h_1(k) + ih_2(k)$ for $i \in (0,N]$ where $N$ is the size of the hashmap.
\end{description}

\subsection{Hash Functions}
Used to map keys to buckets within the hashmap.
\begin{description}
    \item[Compression] Turns the output of a hash function into a bucket index.
        $b(k) = h(k) \mod N$
    \item[MAD] Multiply, Add and Divide.
        Pick a \textsc{prime} $p$ such that $p \geq N, p \geq k$.
        Pick \textsc{random} numbers $\alpha, \beta$ from $(1, p]$.
        $h(k) = 1 + (((\alpha k + \beta) \mod p) \mod N)$
    \item[Polynomial Accumulation] \textsc{Partition} the bits of the key into a sequence of components of fixed length $a$.
        \textsc{Pick} a fixed value $z$.
        \textsc{Evaluate} the polynomial $p(z) = a_0 + a_1z + a_2z^2 + a_3z^3+...$, ignoring overflows.
        $h(k) = p(z)$

\end{description}

\section{String Searching}
Given a pattern of length $p$ and a string of length $s$, find the pattern within the string.
\begin{description}
    \item[Brute Force]
        $O(ps)$
    \item[Boyer Moore]
        $O(ps)$
        \textsc{Compare} pattern from the end backwards. 
        \textsc{Jump} forward until $P[j]=S[i]$ where $i$ is the location in the string that the comparison failed.
    \item[KMP]
        $O(p + s)$
        \textsc{Create} a failure table $F$.
        \textsc{Compare} pattern from the end forwards.
        \textsc{Jump} forward until $j=F[P[i]]$ where $i$ is the location in the string that the comparison failed.
\end{description}

\section{Graphs}

\subsection{Storage}
\begin{description}
    \item[Adjacency Matrix]
        \textbf{O($1$)} finding if two nodes have an edge between them, and adding / removing edges.
        \textbf{O($n$)} finding the edges attached to a node.
        \textbf{O($n^2$)} adding / removing nodes.
    \item[Adjacency List]
        \textbf{O($1$)} adding / removing edges or nodes.
        \textbf{O($e$)} finding the edges attached to a node, and finding if two nodes have an edge between them.
\end{description}

\subsection{Descriptions}
\begin{description}
    \item[Path] sequence of alternating vertices and edges that begins and ends with a vertex.
    \item[Simple Path] path where all vertices and edges are distinct.
    \item[Cycle] path that starts and ends on the same vertex.
    \item[Simple Cycle] all edges distinct, all vertices except the start (and end) distinct.
    \item[Euler Cycle] cycle that visits every edge once.
    \item[Hamiltonian Cycle] cycle that visits every node once.
\end{description}

\subsection{Searches / Traversals}
\begin{description}
    \item[Depth first]
        Uses a stack.
        Detects if directed / undirected graph is cyclic.
        Find strongly connected components.
    \item[Breadth first]
        Uses a queue.
        Find shortest path tree for uniform weight.
        Detects if undirected graph is cyclic.
\end{description}

\section{Pseudocode}
Creating the failure table for KMP algorithm.
\begin{lstlisting}
Algorithm failureFunction(P)
    F[0] = 0
    i = 1
    j = 0
    while i < m
        if P[i] = P[j] then
            F[i] = j + 1
            i++
            j++
        else if j > 0 then
            j = F[j - 1]
        else
            F[i] = 0
            i++
\end{lstlisting}

\end{multicols}
\end{document}
